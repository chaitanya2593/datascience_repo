{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handbook for scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Table of Contents </h1>\n",
    "\n",
    "1. [Introduction](#Intro)\n",
    "2. [Importing the packages](#packages)\n",
    "3. [Import the data set](#Data_set)\n",
    "4. [Perform Descriptive Statistics on the dataset](#EDA)\n",
    "5. [Handling missing values](#Missing_values)     \n",
    "6. [Selecting the columns](#Select_columns)\n",
    "7. [Example Snippets](#snippets)\n",
    "8. [Machine Learning Models](#ML)\n",
    "    - 8.1 [Generic functions](#Generic_functions)\n",
    "    - 8.2 [Linear Regression](#Linear_Regression)\n",
    "    - 8.3 [Logistic Regression](#Logistic_Regression)\n",
    "    - 8.4 [Decision Tree - Classification](#RDT)\n",
    "    - 8.5 [Decision Tree - Regression](#RDTR)\n",
    "    - 8.6.[Random Forest_Classification](#Radom_Forest_Class)\n",
    "    - 8.7.[Random Forest_Regression](#Radom_Forest_Reg)\n",
    "    - 8.8 [K-Means](#K-Means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id=\"Intro\">\n",
    "    The main intention of this document is to help the user when he performing the ML activities on the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the packages  <a id='packages'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn \n",
    " \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove teh printing barrier\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO print multiple outputs in single line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import the data set <a id=\"Data_set\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach by using path \n",
    "your_local_path=r\"\"\n",
    "train_df = pd.read_csv(your_local_path+\"titanic.csv\")\n",
    "\n",
    "#Approach by uploading the fiel to jupyter using read_csv\n",
    "titanic_data = pd.read_csv(\"attachment_titanic_lyst9961.csv\")\n",
    "\n",
    "#Approach from url\n",
    "url = 'https://raw.githubusercontent.com/upxacademy/ML_with_Python/master/Datasets/bikeshare.csv?token=AYxzdiGnjM610dBT7PuwUnUNOmm3bGcvks5ZFDyLwA%3D%3D'\n",
    "bikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n",
    "\n",
    "# Loading data from arff files\n",
    "from scipy.io.arff import loadarff\n",
    "phishing_data_raw = loadarff(\"PhishingData.arff\")\n",
    "\n",
    "\n",
    "\n",
    "## Import the DataSet in colabs\n",
    "from google.colab import drive\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# instat import of teh small \n",
    "# from google.colab import files\n",
    "# upload = files.upload()\n",
    "\n",
    "\n",
    "# import the dataset in azure notebooks\n",
    "from azureml import Workspace\n",
    "ws = Workspace()\n",
    "ds = ws.datasets['bidata_Azure_all_data.csv']\n",
    "frame = ds.to_dataframe()\n",
    "\n",
    "import io\n",
    "emp_data = pd.read_csv(io.BytesIO(upload['attachment_train_lyst4523.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Creating a new Virtual Environment\n",
    "\n",
    "\n",
    "pip3 install virtualenv\n",
    "\n",
    "python -m venv myvenv1\n",
    "\n",
    "myvenv\\Scripts\\activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "pip freeze > requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-0099ef538479>\", line 1, in <module>\n",
      "    from tensorflow.python.keras.preprocessing import image\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\applications\\__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import engine\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\__init__.py\", line 23, in <module>\n",
      "    from tensorflow.python.keras.engine.base_layer import Layer\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 54, in <module>\n",
      "    from tensorflow.python.keras.mixed_precision.experimental import autocast_variable\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\mixed_precision\\experimental\\__init__.py\", line 20, in <module>\n",
      "    from tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer import LossScaleOptimizer\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\mixed_precision\\experimental\\loss_scale_optimizer.py\", line 23, in <module>\n",
      "    from tensorflow.python.keras import optimizers\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adadelta.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 38, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\learning_rate_schedule.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.utils import generic_utils\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\__init__.py\", line 38, in <module>\n",
      "    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras.engine.training import Model\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 42, in <module>\n",
      "    from tensorflow.python.keras import metrics as metrics_module\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 34, in <module>\n",
      "    from tensorflow.python.keras.engine.base_layer import Layer\n",
      "ImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 45, in <module>\n",
      "    from . _api.v2 import compat\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 23, in <module>\n",
      "    from . import v1\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 40, in <module>\n",
      "    from . import experimental\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\experimental\\__init__.py\", line 11, in <module>\n",
      "    from tensorflow.python.ops.control_flow_v2_toggles import output_all_intermediates\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_v2_toggles.py\", line 24, in <module>\n",
      "    from tensorflow.python.ops import control_flow_util_v2\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_util_v2.py\", line 28, in <module>\n",
      "    from tensorflow.python.keras.engine import base_layer_utils\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras import applications\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\applications\\__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import engine\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\__init__.py\", line 23, in <module>\n",
      "    from tensorflow.python.keras.engine.base_layer import Layer\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 54, in <module>\n",
      "    from tensorflow.python.keras.mixed_precision.experimental import autocast_variable\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\mixed_precision\\experimental\\__init__.py\", line 20, in <module>\n",
      "    from tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer import LossScaleOptimizer\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\mixed_precision\\experimental\\loss_scale_optimizer.py\", line 23, in <module>\n",
      "    from tensorflow.python.keras import optimizers\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adadelta.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 38, in <module>\n",
      "    from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\learning_rate_schedule.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.utils import generic_utils\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\__init__.py\", line 38, in <module>\n",
      "    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras.engine.training import Model\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 42, in <module>\n",
      "    from tensorflow.python.keras import metrics as metrics_module\n",
      "  File \"C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 34, in <module>\n",
      "    from tensorflow.python.keras.engine.base_layer import Layer\n",
      "ImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\Users\\xsmaddurve\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "\n",
    "import requests\n",
    "from io import BreytesIO    \n",
    "response = requests.get('https://djangostaticsv.blob.core.windows.net/media/uploads/kakra.jpg')\n",
    "# response = requests.get(path_image)\n",
    "img = image.load_img(BytesIO(response.content), target_size=(299, 299)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple code to plot the Matplot lib graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(total_time, list(range(1000,10000,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD CLassififer\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=40000, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42, tol = 0.001, max_iter = 40000,loss=\"log\")\n",
    "sgd_clf.fit(train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 8, 3, 8, 2, 8, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = train.iloc[:10]\n",
    "sample_labels = train_labels.iloc[:10]\n",
    "sgd_clf.predict(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15969    2\n",
       "25018    3\n",
       "242      1\n",
       "5633     2\n",
       "49199    8\n",
       "3253     2\n",
       "48638    8\n",
       "12130    2\n",
       "19662    3\n",
       "14200    2\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(labelsTrue,predictions1,predictions2):\n",
    "    if len(predictions1)>0:\n",
    "        print(\"Accuracy = \" , accuracy_score(labelsTrue,predictions1))\n",
    "        print(\"Precision = \" ,precision_score(labelsTrue,predictions1))\n",
    "        print(\"Recall = \" ,recall_score(labelsTrue,predictions1))\n",
    "        f1 = f1_score(labelsTrue,predictions1,average=\"weighted\")\n",
    "        \n",
    "        print(\"F1 score: \",f1)\n",
    "        \n",
    "        df_confusion_rf = metrics.confusion_matrix(labelsTrue,predictions1)\n",
    "        df_confusion_rf\n",
    "        cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "        sns.heatmap(df_confusion_rf, cmap = cmap,xticklabels=['Prediction No','Prediction Yes'],yticklabels=['Actual No','Actual Yes'], annot=True,\n",
    "            fmt='d')\n",
    "    if len(predictions2)>0:\n",
    "        logloss = log_loss(labelsTrue,predictions2, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "        print(\"Log loss for predicted probabilities:\",logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fbf04dd799e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msgd_prediction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msgd_prediction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msgd_prediction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msgd_prediction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sgd_clf' is not defined"
     ]
    }
   ],
   "source": [
    "sgd_prediction1 = sgd_clf.predict(validation)\n",
    "sgd_prediction2 = sgd_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,sgd_prediction1,sgd_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 6, ..., 9, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.67007350e-02, 3.56206619e-01, 1.28286708e-01, ...,\n",
       "        9.12044210e-02, 7.19499319e-02, 7.02669005e-03],\n",
       "       [5.00395132e-04, 5.40045593e-03, 1.74563619e-01, ...,\n",
       "        8.08774805e-01, 1.19719263e-04, 7.09108120e-04],\n",
       "       [6.03978511e-03, 5.00751749e-02, 2.45544009e-02, ...,\n",
       "        1.15067683e-02, 1.36461970e-03, 1.77577352e-04],\n",
       "       ...,\n",
       "       [5.73666441e-03, 1.65011036e-06, 1.16990833e-02, ...,\n",
       "        2.19330566e-07, 1.89572457e-02, 9.63604748e-01],\n",
       "       [1.07518227e-04, 6.78615096e-01, 1.54603235e-01, ...,\n",
       "        4.81073702e-03, 1.81820568e-02, 1.67413454e-03],\n",
       "       [5.76406412e-03, 4.74393341e-02, 1.94121277e-02, ...,\n",
       "        3.94936523e-01, 1.43414159e-02, 1.42419809e-05]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_prediction2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7739562191425366\n",
      "Log loss for predicted probabilities: 2.3086776076579794\n"
     ]
    }
   ],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(train,train_labels)\n",
    "knn_prediction1 = knn_clf.predict(validation)\n",
    "knn_prediction2 = knn_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,knn_prediction1,knn_prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8000220971559647\n",
      "Log loss for predicted probabilities: 0.5982023119280968\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "rf_clf.fit(train,train_labels)\n",
    "rf_prediction1 = rf_clf.predict(validation)\n",
    "rf_prediction2 = rf_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,rf_prediction1,rf_prediction2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8024313001815613\n",
      "Log loss for predicted probabilities: 0.6747438247280982\n"
     ]
    }
   ],
   "source": [
    "trees_clf = ExtraTreesClassifier(n_estimators=50,n_jobs=-1)\n",
    "trees_clf.fit(train,train_labels)\n",
    "prediction1_trees = trees_clf.predict(validation)\n",
    "prediction2_trees = trees_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,prediction1_trees,prediction2_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7842698381041376\n",
      "Log loss for predicted probabilities: 0.6845708245243889\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier()\n",
    "mlp_clf.fit(train,train_labels)\n",
    "prediction1_mlp = mlp_clf.predict(validation)\n",
    "prediction2_mlp = mlp_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,prediction1_mlp,prediction2_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.6744072921883629\n",
      "Log loss for predicted probabilities: 2.0261936531308335\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier()\n",
    "ada_clf.fit(train,train_labels)\n",
    "prediction1_ada = ada_clf.predict(validation)\n",
    "prediction2_ada = ada_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,prediction1_ada,prediction2_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.6286302230265768\n",
      "Log loss for predicted probabilities: 7.251880092416661\n"
     ]
    }
   ],
   "source": [
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train,train_labels)\n",
    "prediction1_nb = nb_clf.predict(validation)\n",
    "prediction2_nb = nb_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,prediction1_nb,prediction2_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejks\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7578247463382258\n",
      "Log loss for predicted probabilities: 0.6483268010940888\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(num_class = 9,objective=\"multi:softprob\",eval_metric=\"mlogloss\")\n",
    "xgb_clf.fit(train,train_labels)\n",
    "prediction1_xgb = xgb_clf.predict(validation)\n",
    "prediction2_xgb = xgb_clf.predict_proba(validation)\n",
    "evaluate(validation_labels,prediction1_xgb,prediction2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, eval_metric='mlogloss', gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, num_class=9, objective='multi:softprob',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=42, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [10], 'n_estimators': [300], 'gamma': [0.03], 'learning_rate': [0.08], 'min_child_weight': [5], 'colsample_bytree': [0.8], 'subsample': [0.85]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_log_loss', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs_clf = XGBClassifier(num_class = 9,\n",
    "                           objective=\"multi:softprob\",\n",
    "                           eval_metric=\"mlogloss\",\n",
    "                           seed=42)                         \n",
    "                        \n",
    "param_grid = {\"max_depth\": [10,15,20,30],\n",
    "              \"n_estimators\": [300,400,500] , \n",
    "              \"gamma\": [0.03,0.05], \n",
    "              \"learning_rate\": [0.08,0.09],\n",
    "              \"min_child_weight\": [5,10], \n",
    "              \"colsample_bytree\": [0.4,0.8], \n",
    "              \"subsample\": [0.50,0.85]} \n",
    "\n",
    "grid_search = GridSearchCV(xgb_gs_clf, \n",
    "                           param_grid=param_grid,\n",
    "                           cv = 2,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='neg_log_loss',\n",
    "                           verbose=2)\n",
    "grid_search.fit(train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'gamma': 0.03, 'learning_rate': 0.08, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 300, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejks\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8249506188319765\n",
      "Log loss for predicted probabilities: 0.4502995159397517\n"
     ]
    }
   ],
   "source": [
    "grid_prediction1 = grid_search.predict(validation)\n",
    "grid_prediction2 = grid_search.predict_proba(validation)\n",
    "evaluate(validation_labels,grid_prediction1,grid_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = grid_search.best_estimator_\n",
    "xgb_prediction1 = grid_prediction1\n",
    "xgb_prediction2 = grid_prediction2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>NearestNeighbours</th>\n",
       "      <th>NeuralNetworrk</th>\n",
       "      <th>ExtraTrees</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>SGD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  xgboost  NearestNeighbours  NeuralNetworrk  ExtraTrees  \\\n",
       "0             2        7                  2               7           2   \n",
       "1             7        7                  7               7           7   \n",
       "2             6        6                  6               6           6   \n",
       "3             6        6                  6               4           6   \n",
       "4             6        6                  6               6           6   \n",
       "\n",
       "   AdaBoost  NaiveBayes  SGD  \n",
       "0         2           4    2  \n",
       "1         1           3    7  \n",
       "2         6           6    6  \n",
       "3         4           9    4  \n",
       "4         6           6    6  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame( {'RandomForest': rf_prediction1,\n",
    "                                  'xgboost': xgb_prediction1,\n",
    "                                  'NearestNeighbours': knn_prediction1,\n",
    "                                  'NeuralNetworrk': prediction1_mlp,\n",
    "                                  'ExtraTrees': prediction1_trees,\n",
    "                                  'AdaBoost': prediction1_ada,\n",
    "                                  'NaiveBayes': prediction1_nb,\n",
    "                                  'SGD':sgd_prediction1,\n",
    "                        \n",
    "    })\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1_clf = mlp_clf\n",
    "base2_clf = rf_clf\n",
    "base3_clf = knn_clf\n",
    "base4_clf = ada_clf\n",
    "base5_clf = sgd_clf\n",
    "final_clf = xgb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f07363357739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mn_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_class = len(classes)\n",
    "kf = KFold(n_splits= n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((x_train.shape[0],n_class))\n",
    "    oof_test = np.zeros((ntest,n_class))\n",
    "    oof_test_temp = np.empty((n_folds, ntest))\n",
    "   \n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        \n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "  \n",
    "        \n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        pred_te = clf.predict_proba(x_te)\n",
    "        oof_train[test_index,:] = pred_te\n",
    "        \n",
    "        pred_test = clf.predict_proba(x_test)\n",
    "        oof_test += pred_test\n",
    "\n",
    "    return oof_train, oof_test/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.values\n",
    "x_test = validation.values\n",
    "y_train = train_labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1_oof_train, base1_oof_test = get_oof(base1_clf, x_train, y_train, x_test)\n",
    "base2_oof_train, base2_oof_test = get_oof(base2_clf, x_train, y_train, x_test)\n",
    "base3_oof_train, base3_oof_test = get_oof(base3_clf, x_train, y_train, x_test)\n",
    "base4_oof_train, base4_oof_test = get_oof(base4_clf, x_train, y_train, x_test) \n",
    "base5_oof_train, base5_oof_test = get_oof(base5_clf, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((base1_oof_train, \n",
    "                          base2_oof_train,\n",
    "                          base3_oof_train,\n",
    "                          base4_oof_train,\n",
    "                          base5_oof_train), axis=1)\n",
    "x_test = np.concatenate((base1_oof_test,\n",
    "                         base2_oof_test,\n",
    "                         base3_oof_test,\n",
    "                         base4_oof_test,\n",
    "                         base5_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, eval_metric='mlogloss', gamma=0.03,\n",
       "       learning_rate=0.08, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=5, missing=None, n_estimators=300, n_jobs=1,\n",
       "       nthread=None, num_class=9, objective='multi:softprob',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=42, silent=True, subsample=0.85)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tejks\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8275882375811346\n",
      "Log loss for predicted probabilities: 0.44972287818652373\n"
     ]
    }
   ],
   "source": [
    "prediction1_stacked = final_clf.predict(x_test)\n",
    "prediction2_stacked = final_clf.predict_proba(x_test)\n",
    "evaluate(validation_labels,prediction1_stacked,prediction2_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========== ENd of Stacking  ============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "import tensorflow.python.keras as tf\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from pyod.models.auto_encoder import AutoEncoder \n",
    "clf1 = AutoEncoder(hidden_neurons =[30, 10,2, 10, 30], epochs=100)\n",
    "clf1.fit(X_train)\n",
    "\n",
    "# Get the outlier scores for the train data\n",
    "y_train_scores = clf1.decision_scores_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "outliers_fraction = i\n",
    "#contamination=outliers_fraction,\n",
    "xx , yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "clf = IsolationForest(contamination=outliers_fraction,random_state=32 , bootstrap=False, \n",
    "                max_features=X_triner_minmax.shape[1], max_samples='auto', n_estimators=500,\n",
    "                n_jobs=-1,)\n",
    "clf.fit(X_triner_minmax)\n",
    "scores_pred = clf.decision_function(X_triner_minmax) * -1\n",
    "\n",
    "y_pred = clf.predict(X_triner_minmax)\n",
    "\n",
    "\n",
    "\n",
    "X_train_minmax.shape[1]\n",
    "\n",
    "\n",
    "final_df['Outliers'] = y_pred\n",
    "final_df['Outliers'].replace({1:0,-1:1}, inplace=True)\n",
    "final_df['Outliers'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate(final_df['ICCC Results'],final_df['Outliers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn Pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "clf = KNN(contamination=0.01)\n",
    "clf.fit(nor_final_df_km_pca)\n",
    "scores_pred = clf.decision_function(nor_final_df_km_pca) * -1\n",
    "y_pred = clf.predict(nor_final_df_km_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open('MCMC.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(xgb_clf, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('oct_mar_xgb_clf_f_all_ER_v1.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "#     xgb_clf = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "import socket\n",
    "socket.getaddrinfo('localhost', 8080)\n",
    "port = 465  # For SSL\n",
    "# password = input(\"Type your password and press enter: \")\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "receiver_email = sender_email = \"sundar2593@gmail.com\"  # Enter your address\n",
    "# receiver_email = \"venkata.madduri@xs.nestle.com\"  # Enter receiver address\n",
    "\n",
    "\n",
    "# Create a secure SSL context\n",
    "# context = ssl.create_default_context()\n",
    "\n",
    "message = \"\"\"\\\n",
    "Subject: Hi there\n",
    "\n",
    "This message is sent from Python.\"\"\"\n",
    "\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "    server.login(sender_email, \"sundar@1\")\n",
    "    \n",
    "    try:\n",
    "      server.sendmail(sender_email, receiver_email, message)\n",
    "      print(\"messgae sent successfully\")\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(\"Error: unable to send email\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "part2 = MIMEText(soup, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook Mail configuration , sending and extracting mails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exchangelib import Configuration, Account, DELEGATE\n",
    "from exchangelib import Message, Mailbox, FileAttachment\n",
    "\n",
    "\n",
    "credentials = Credentials(\n",
    "    username='venkata.madduri@xs.nestle.com',  # Or myusername@example.com for O365\n",
    "    password=''\n",
    ")\n",
    "account = Account(\n",
    "    primary_smtp_address='venkata.madduri@xs.nestle.com', \n",
    "    credentials=credentials, \n",
    "    autodiscover=True, \n",
    "    access_type=DELEGATE\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "recipients = ['venkata.madduri@xs.nestle.com']\n",
    "for recipient in recipients:\n",
    "    to_recipients.append(Mailbox(email_address=recipient))\n",
    "# Create message\n",
    "m = Message(account=account,\n",
    "            folder=account.sent,\n",
    "            subject=\"Outlook Test Email\",\n",
    "            body=\"It works\",\n",
    "            to_recipients=to_recipients)\n",
    "\n",
    "# attach files\n",
    "attachments = []\n",
    "for attachment_name, attachment_content in attachments or []:\n",
    "    file = FileAttachment(name=attachment_name, content=attachment_content)\n",
    "    m.attach(file)\n",
    "m.send_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 100 inbox messages in reverse order\n",
    "# for item in account.inbox.all().order_by('-datetime_received')[:20]:\n",
    "#     print(item.subject, item.body, item.attachments)\n",
    "\n",
    "# specific email extract friom sent items\n",
    "item = account.sent.get(subject='Ticket booking') # subject must be extact or us regex\n",
    "item.body\n",
    "\n",
    "# to convert str html to exchangelib HTML\n",
    "import exchangelib\n",
    "soup_ht = exchangelib.properties.HTMLBody(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML extracte and operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "\n",
    "def read_xml(xml_file):\n",
    "        with open(xml_file, 'r') as f:\n",
    "                data = f.read()\n",
    "        return parseString(data)\n",
    "\n",
    "dom = read_xml(r\"C:\\Users\\xsmaddurve\\OneDrive - NESTLE\\Prioritize\\Sample.html\")\n",
    "\n",
    "for x in dom.getElementsByTagName('span'):\n",
    "\n",
    "    print(x.toxml())\n",
    "\n",
    "#child node extraction \n",
    "dom.getElementsByTagName('span')[3].childNodes[0].nodeValue = \"shdshd\"\n",
    "dom.getElementsByTagName('span')[3].childNodes[0].nodeValue\n",
    "dom.getElementsByTagName('span')[5].childNodes[0].nodeValue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML extraction using xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "\n",
    "tree = ET.parse(r\"C:\\Users\\xsmaddurve\\OneDrive - NESTLE\\Prioritize\\Sample.html\")\n",
    "xml_data = tree.getroot()\n",
    "#here you can change the encoding type to be able to set it to the one you need\n",
    "xmlstr = ET.tostring(xml_data, encoding='utf-8', method='xml')\n",
    "\n",
    "data_dict = dict(xmltodict.parse(xmlstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_data\n",
    "for elem in tree:\n",
    "    for subelem in elem.findall('span'):\n",
    "    \n",
    "        # if we don't need to know the name of the attribute(s), get the dict\n",
    "        print(subelem.attrib)      \n",
    "    \n",
    "        # if we know the name of the attribute, access it directly\n",
    "#         print(subelem.get('name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending an mail form outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exchangelib import Configuration, Account, DELEGATE, Credentials\n",
    "from exchangelib import Message, Mailbox, FileAttachment, properties\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#label,\n",
    "def compose_mail(incident, caller, created, priority, description, short_description, label,  business_sevice):\n",
    "    xml_file = r\"/content/Sample.html\"\n",
    "    with open(xml_file, 'r') as f:\n",
    "        data = f.read()\n",
    "#     data=item.body\n",
    "    soup = BeautifulSoup(data,'lxml')\n",
    "    all_links = soup.find_all(\"span\")\n",
    "    for link in range(0,len(all_links)):\n",
    "\n",
    "        if 'Incident number' in  all_links[link].text:           \n",
    "            all_links[link + 1].string = incident\n",
    "            continue\n",
    "        if 'Caller' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = caller\n",
    "            continue\n",
    "        if 'Opened date' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = str(created)\n",
    "            continue\n",
    "        if 'ML Priority' in  all_links[link].text: \n",
    "            print()\n",
    "            all_links[link + 1].string = str(label)\n",
    "            continue\n",
    "        if 'Priority' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = priority\n",
    "            continue\n",
    "\n",
    "        if 'Business service' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = business_sevice\n",
    "            continue\n",
    "        if 'Short description' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = short_description\n",
    "            continue\n",
    "        if 'Description' in  all_links[link].text:        \n",
    "            all_links[link + 1].string = description\n",
    "            continue\n",
    "            \n",
    "    soup_html = properties.HTMLBody(soup)\n",
    "    subject  = incident + ' | SNOW priority '+ priority + ' | Business Priority ' + str(label) + ' |' + business_sevice\n",
    "    return soup_html, subject\n",
    "\n",
    "def send_email(body, subject, sender_list):\n",
    "    \n",
    "    try:\n",
    "        credentials = Credentials(\n",
    "          username='Venkata.Madduri@XS.nestle.com',  # Or myusername@example.com for O365\n",
    "          password='Sundar@001'\n",
    "        )\n",
    "        config = Configuration(server='outlook.office.com', credentials=credentials)\n",
    "\n",
    "        account = Account(\n",
    "          primary_smtp_address='Venkata.Madduri@XS.nestle.com', \n",
    "          credentials=credentials, \n",
    "          autodiscover=True, \n",
    "          access_type=DELEGATE,\n",
    "          # config=config,\n",
    "        )\n",
    "\n",
    "        to_recipients = []\n",
    "        cc_recipients_final = []\n",
    "#         recipients = ['BheeshmaKaradi.Somashekharappa@xs.nestle.com','Venkata.Madduri@XS.nestle.com']\n",
    "        recipients = sender_list\n",
    "        for recipient in recipients:\n",
    "            to_recipients.append(Mailbox(email_address=recipient))\n",
    "        \n",
    "        cc_recipients = ['BheeshmaKaradi.Somashekharappa@xs.nestle.com','Venkata.Madduri@XS.nestle.com']\n",
    "        for cc_recipient in cc_recipients:\n",
    "            cc_recipients_final.append(Mailbox(email_address=cc_recipient))\n",
    "        \n",
    "        # Create message\n",
    "        m = Message(account=account,\n",
    "                  folder=account.sent,\n",
    "                  subject= subject,\n",
    "                  body= body, \n",
    "                  to_recipients=to_recipients,\n",
    "                   cc_recipients=cc_recipients_final)\n",
    "\n",
    "        # attach files\n",
    "        '''with open('Auto_priority_tickets.pdf', 'rb') as f:\n",
    "          content = f.read()\n",
    "        attachments.append(('Auto_priority_tickets.pdf', content))'''\n",
    "\n",
    "        attachments = []\n",
    "        print(\"Checkpoit 1\")\n",
    "        for attachment_name, attachment_content in attachments or []:\n",
    "            file = FileAttachment(name=attachment_name, content=attachment_content)\n",
    "            m.attach(file)\n",
    "        m.send_and_save()\n",
    "        print(\"Sucessfully Triggered the mail to \" + str(sender_list))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #set the flag\n",
    "\n",
    "\n",
    "    \n",
    "def send_mail_ml_priorized_records(incidents_df, ag_dataframe):\n",
    "    incidents_df['ML_criteria'].fillna('', inplace=True)\n",
    "    sender_dataframe = incidents_df[(incidents_df['ML_criteria']!='No Filter Satisfied') & (incidents_df['ML_criteria']!='')].head(3)\n",
    "    sender_dataframe.index = np.arange(1, len(sender_dataframe) + 1)\n",
    "    for counter in range(1,len(sender_dataframe)+1):\n",
    "        incident = str(sender_dataframe.loc[counter]['Number'])\n",
    "        caller = str(sender_dataframe.loc[counter]['Caller'])\n",
    "        created = str(sender_dataframe.loc[counter]['Created'])\n",
    "        priority = str(sender_dataframe.loc[counter]['Priority'])\n",
    "        description = str(sender_dataframe.loc[counter]['Description'])\n",
    "        short_description = str(sender_dataframe.loc[counter]['Short_description'])\n",
    "        label = sender_dataframe.loc[counter]['ML_Priority']\n",
    "        business_sevice = str(sender_dataframe.loc[counter]['Business_service'])               \n",
    "        ag_name = sender_dataframe.loc[counter]['Assignment_group']              \n",
    "        sender_list = list(ag_dataframe[ag_dataframe['service_now']== ag_name]['email_id'].values)        \n",
    "        # compose mail \n",
    "        mail_html, subject = compose_mail(incident, caller, created, priority, description, short_description, label,  business_sevice)        \n",
    "        # Sending mail to the AG specified  \n",
    "        send_email(mail_html , subject, sender_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform the training using KFolds cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the boston dataset\n",
    "data = datasets.load_boston()\n",
    " \n",
    "# extract the predictors and target data.\n",
    "predictors = data.data\n",
    "target = data.target\n",
    "\n",
    "\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "cv_r2_scores_rf = cross_val_score(rf_reg, predictors, target, cv=5,scoring='r2')\n",
    "print(cv_r2_scores_rf)\n",
    "print(\"Mean 5-Fold R Squared: {}\".format(np.mean(cv_r2_scores_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "rf_reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = []\n",
    "for i in range(5):\n",
    "    result = next(kf.split(df), None)\n",
    "    x_train = df.iloc[result[0]]\n",
    "    x_test = df.iloc[result[1]]\n",
    "    y_train = y.iloc[result[0]]\n",
    "    y_test = y.iloc[result[1]]\n",
    "    model = rf_reg.fit(x_train,y_train)\n",
    "    predictions = rf_reg.predict(x_test)\n",
    "    scores.append(model.score(x_test,y_test))\n",
    "print('Scores from each Iteration: ', scores)\n",
    "print('Average K-Fold Score :' , np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
